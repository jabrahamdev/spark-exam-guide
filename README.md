# Guide

## 1.- Spark is developed in which language?

Python

R

Java

**Scala**

**Spark is developed primarily in Scala. Scala is a programming language that runs on the Java Virtual Machine (JVM) and provides a concise and expressive syntax. While Spark supports multiple programming languages, including Python, R, and Java, its core implementation is in Scala.** 

## 2.- In Spark Streaming the data can be from what all sources?

Kafka

Flume

Kinesis

**All of the Above**

**In Spark Streaming, data can be ingested from various sources, including Kafka, Flume, and Kinesis. Spark Streaming provides built-in connectors or receivers for these popular streaming platforms, making it easier to consume data from them. Therefore, the correct answer is "All of the Above." Spark Streaming offers flexibility and interoperability with multiple data sources to accommodate different streaming data ingestion requirements.**


## 3.- Apache Spark has APIâ€™s in

Java

Scala

Python

**All of the above**

**Apache Spark provides APIs in Java, Scala, and Python. These three languages are supported by Spark, allowing developers to choose their preferred language for interacting with Spark's functionality and building Spark applications. The Spark APIs are designed to provide a consistent experience across these languages, enabling developers to leverage the power of Spark using the programming language they are most comfortable with. **

## 4.- Which of the following is not a component of Spark Ecosystem?

**Sqoop**

GraphX

MLlib

BlinkDB


**Sqoop is not a component of the Spark ecosystem. Sqoop is actually a tool developed by Apache for efficiently transferring bulk data between Apache Hadoop and structured data stores such as relational databases. On the other hand, GraphX, MLlib, and BlinkDB are all components of the Spark ecosystem.**

**GraphX is a graph processing library built on top of Spark that provides a distributed computation framework for graph analytics. MLlib, also known as Spark ML, is a machine learning library that offers various algorithms and tools for data analysis and machine learning tasks. BlinkDB, although an experimental component, is a query engine designed for interactive big data analysis and exploration within Spark.**



## 5.- The basic abstraction of Spark Streaming is

**Dstream**

RDD

Shared Variable

None of the above


**The basic abstraction of Spark Streaming is DStream (Discretized Stream). DStream represents a continuous stream of data in Spark Streaming. It is a sequence of RDDs (Resilient Distributed Datasets), where each RDD contains data from a specific interval of time.**

**DStream provides a high-level API for performing transformations and actions on the streaming data. It enables developers to apply various operations, such as filtering, mapping, reducing, and joining, on the data stream. DStreams abstract away the complexity of handling continuous data streams and provide a programming interface similar to working with batch data in Spark RDDs.**


**DStreams can either be created from live data (such as, data from HDFS, Kafka or Flume) or it can be generated by transformationexisting DStreams using operations such as map, window and reduceByKeyAndWindow.**

**Internally, there are few basic properties by which DStreams is characterized:**

1. DStream depends on the list of other DStreams.
2. A time interval at which the DStream generates an RDD
3. A function that is used to generate an RDD after each time interval

## 6.- Which of the following algorithm is not present in MLlib?

Streaming Linear Regression

Streaming KMeans

**Tanimoto distance**

None of the above


**The algorithm that is not present in MLlib (Spark's machine learning library) is Tanimoto distance.**

**MLlib offers a wide range of machine learning algorithms and utilities for various tasks such as classification, regression, clustering, and collaborative filtering. However,  MLlib does not include the Tanimoto distance algorithm.**

**Streaming Linear Regression and Streaming KMeans, on the other hand, are supported algorithms in MLlib for handling streaming data in real-time scenarios.**


## 7.-  Dstream internally is

**Continuous Stream of RDD**

Continuous Stream of DataFrame

Continuous Stream of DataSet

None of the above



**DStream internally represents a continuous stream of RDDs (Resilient Distributed Datasets). Therefore, the correct answer is "Continuous Stream of RDD." DStream is a fundamental abstraction in Spark Streaming, and it provides a high-level programming interface for processing and manipulating streaming data. It allows developers to apply transformations and actions on the stream of RDDs, enabling real-time data processing and analytics in Spark Streaming.**


## 8.- Can we add or setup new string computation after SparkContext starts

Yes

**No**


**We cannot add or set up new string computation after the SparkContext starts.**

**Once the SparkContext is initialized and starts running, it represents the entry point for interacting with a Spark cluster and coordinating the execution of Spark applications. It sets up the environment and configuration for the Spark application, including resources allocation, task scheduling, and communication with the cluster.**

**Any configuration or computation that needs to be set up or added should be done before starting the SparkContext. Once the SparkContext is running, it is not possible to modify its configuration or add new computation tasks dynamically.**

**If you need to make changes to the computation or configuration, you would typically need to stop the SparkContext and restart it with the desired changes.**


## 10.- Which is the abstraction of Apache Spark?

Shared Variable

**RDD**

Both the above


**The abstraction of Apache Spark is the Resilient Distributed Dataset (RDD). RDD is a fundamental data structure in Spark that represents an immutable distributed collection of objects. RDDs provide a fault-tolerant and efficient way to perform distributed data processing tasks across a cluster of machines.**

**RDDs enable distributed transformations and actions in Spark, allowing users to perform various operations such as filtering, mapping, reducing, joining, and more on large datasets. RDDs are resilient because they automatically recover from node failures and provide fault tolerance through lineage, which allows them to be reconstructed in case of failures.**

**Shared variables, on the other hand, are a mechanism in Spark to share variables across tasks or nodes in a distributed computing environment. While shared variables are essential for certain computations in Spark, RDDs are the primary abstraction and foundation for data processing in Apache Spark.**




## 11.- What are the parameters defined to specify window operation

**Window length, sliding interval**

State size, window length

State size, sliding interval

None of the above


**The parameters defined to specify a window operation in Apache Spark are the window length and the sliding interval.**

**Window Length: It determines the size or length of the window, which is the duration or number of elements included in each window. It specifies the range of data to be considered in the window operation.**

**Sliding Interval: It specifies the interval or step size at which the window moves or slides over the data stream. It determines how often the window operation is performed and the overlap between consecutive windows.**

**By specifying these two parameters, you can define the size and movement of the window for performing computations or aggregations on the streaming data.**

**The correct answer is "Window length, sliding interval."**


## 12.- Which of the following is not output operation on DStream

SaveAsTextFiles

ForeachRDD

SaveAsHadoopFiles

**ReduceByKeyAndWindow**


**"ReduceByKeyAndWindow" is not an output operation on DStream.**

**In Spark Streaming, DStream provides various output operations that allow you to write or process the data in the DStream.**

**"SaveAsTextFiles" is an output operation that saves the DStream's data to text files.**

**"ForeachRDD" is an output operation that allows you to apply a function to each RDD in the DStream. This function can perform custom processing or write the data to external systems.**

**"SaveAsHadoopFiles" is an output operation that saves the DStream's data to Hadoop files using the older Hadoop API.**

**On the other hand, "ReduceByKeyAndWindow" is not an output operation. It is a transformation operation that performs reduction on key-value pairs within a sliding window.**

**The correct answer is "ReduceByKeyAndWindow."**

## 13.-  Dataset was introduced in which Spark release?

**Spark 1.6**

Spark 1.4.0

Spark 2.1.0

Spark 1.1


**The Dataset API was introduced in Spark 1.6. It was a significant addition to Spark, providing a higher-level abstraction than RDDs (Resilient Distributed Datasets) while offering strong typing and optimization benefits.**

**Prior to Spark 1.6, Spark primarily provided RDDs as the core data abstraction for distributed computing. However, the Dataset API brought a more structured and efficient way to work with data by combining the best features of RDDs and DataFrames.**

## 14.- Which Cluster Manager do Spark Support?

Standalone Cluster Manager

MESOS

YARN

**All of the above**

**Spark supports all of the mentioned cluster managers: Standalone Cluster Manager, Mesos, and YARN.**

**Standalone Cluster Manager: Spark has its own built-in standalone cluster manager, which allows you to deploy and manage Spark applications on a cluster without relying on any external resource manager.**

**Mesos: Spark can also be run on Apache Mesos, a general-purpose cluster manager that provides resource isolation and sharing across distributed applications.**

**YARN: Spark can integrate with Apache Hadoop YARN (Yet Another Resource Negotiator), which is a cluster management technology used in Hadoop ecosystem. Spark applications can be deployed and executed on YARN, leveraging its resource management capabilities.**

## 15.- The default storage level of cache() is?

**MEMORY_ONLY**

MEMORY_AND_DISK

DISK_ONLY

MEMORY_ONLY_SER


**The default storage level of the cache() operation in Apache Spark is MEMORY_ONLY.**

**When you call cache() on a DataFrame or RDD in Spark, it caches the data in memory for faster access during subsequent operations. The default storage level ensures that the data is stored in memory as deserialized Java objects.**

**However, it's important to note that the default storage level can be changed by modifying the spark.storage.memoryFraction configuration parameter in Spark's configuration. This parameter determines the fraction of Java heap space allocated for Spark's storage cache.**

**If you explicitly want to change the storage level while caching, you can use the persist() method and pass the desired storage level as a parameter. For example, you can use persist(StorageLevel.DISK_ONLY) to cache the data only on disk.**

**But by default, without explicitly specifying the storage level, the data will be cached in memory using the MEMORY_ONLY storage level.**

## 16.- Which is not a component on the top of Spark Core?

**Spark RDD**

Spark Streaming

MLlib

None of the above


17.- Apache Spark was made open-source in which year?

**2010**

2011

2008

2009


**Apache Spark was made open-source in the year 2010.**

## 18.- In addition to stream processing jobs, what all functionality do Spark provides?

Machine learning

Graph processing

Batch processing

**All of the above**


**All of the above functionalities are provided by Spark in addition to stream processing jobs.**

**Machine Learning: Spark provides a machine learning library called MLlib, which offers a wide range of machine learning algorithms and tools. It enables developers to perform various tasks such as classification, regression, clustering, recommendation, and more using distributed computing.**

**Graph Processing: Spark provides a graph processing library called GraphX. It allows developers to perform graph computations and analysis, including graph algorithms, graph querying, and graph visualization, on large-scale graphs using distributed processing capabilities.**

**Batch Processing: Spark is known for its ability to handle batch processing workloads efficiently. It allows developers to process large volumes of data in parallel using RDDs and DataFrame/Dataset APIs. Spark optimizes the execution of batch jobs and provides a rich set of transformations and actions for data manipulation and analysis.**



## 19.-  Is Spark included in every major distribution of Hadoop?

Yes

**No**

**No, Spark is not included in every major distribution of Hadoop by default. While Spark can be integrated with Hadoop and run on top of Hadoop clusters, it is not a mandatory component of every Hadoop distribution.**

**Hadoop distributions typically include the core components of the Hadoop ecosystem, such as the Hadoop Distributed File System (HDFS) and the MapReduce processing framework. However, Spark is a separate and independent project from Hadoop, and it needs to be installed and configured separately on a Hadoop cluster if you want to use it alongside Hadoop.**

**That being said, many Hadoop distributions, such as Cloudera, Hortonworks, and MapR, do provide built-in support for Spark. They may offer pre-packaged installations or provide integration guides to help users easily set up and run Spark on their Hadoop clusters. However, it ultimately depends on the specific Hadoop distribution and its offerings.**

**Therefore, the correct answer is "No." While Spark can be integrated with and run on Hadoop clusters, it is not included in every major distribution of Hadoop by default.**

## 20.- Which of the following is not true for Hadoop and Spark?

Both are data processing platforms

Both are cluster computing environments

**Both have their own file system**

Both use open source APIs to link between different tools


**The statement "Both have their own file system" is not true for Hadoop and Spark.**

**Hadoop and Spark are both data processing platforms and cluster computing environments, but they differ in terms of their file systems.**

**Hadoop, specifically the Apache Hadoop ecosystem, includes the Hadoop Distributed File System (HDFS) as its primary storage layer. HDFS is a distributed file system designed for storing and processing large datasets acrosQue tas a cluster of commodity hardware. It provides fault tolerance, high throughput, and scalability.**

**On the other hand, Spark does not have its own dedicated file system. Instead, it is designed to be compatible with various storage systems, including HDFS, Amazon S3, Azure Data Lake Storage, and more. Spark can read and process data from different file systems and data sources, utilizing their specific optimizations and features.**

**Regarding the statement "Both use open source APIs to link between different tools," it is true. Both Hadoop and Spark are open-source projects and provide open-source APIs that allow integration with various tools and frameworks within their respective ecosystems. These APIs enable developers to connect and interact with different components, libraries, and tools available in the Hadoop and Spark ecosystems.**

**The correct answer is "Both have their own file system."**


## 21.- How much faster can Apache Spark potentially run batch-processing programs when processed in memory than MapReduce can?

10 times faster

20 times faster

**100 times faster**

200 times faster


**Apache Spark can potentially run batch-processing programs much faster in memory compared to MapReduce. While the actual performance gain depends on various factors, Spark is generally known to be significantly faster than MapReduce for in-memory processing.**

**The common estimation is that Apache Spark can be up to 100 times faster than MapReduce for batch-processing programs when processed in memory. This substantial performance improvement is mainly due to Spark's ability to cache data in memory and perform in-memory computations, avoiding costly disk I/O operations.**

**It's important to note that the performance gain can vary depending on the specific workload, data size, cluster configuration, and the efficiency of the Spark program itself. However, Spark's in-memory processing capability is one of its key advantages, enabling faster data processing and iterative computations.**

**The closest option is "100 times faster." Apache Spark can potentially offer a significant speedup compared to MapReduce when batch-processing programs are processed in memory.**


## 22.-  Which of the following provide the Spark Coreâ€™s fast scheduling capability to perform streaming analytics.

RDD

GraphX

**Spark Streaming**

Spark R

**Spark Streaming provides the fast scheduling capability to perform streaming analytics using Spark Core. It is an extension of Spark Core specifically designed for processing and analyzing real-time streaming data.**

**RDD (Resilient Distributed Datasets) is the core data abstraction in Spark, representing distributed collections of objects that can be processed in parallel. RDDs are used in Spark Streaming as the underlying data structure for processing streaming data, but they *do not* directly provide the fast scheduling capability.**

**GraphX is a graph processing library in Spark that allows performing graph computations. While it is a component built on top of Spark Core, it is not directly related to the scheduling capability for streaming analytics.**

**Spark R is an R package that provides an interface for running Spark computations using the R programming language. While it enables using Spark with R, it is not specifically responsible for the fast scheduling capability in Spark Core for streaming analytics.**

**The correct option is "Spark Streaming." It provides the necessary functionality and scheduling capabilities within Spark Core for performing streaming analytics on real-time data.**

## 23.- Which of the following is the reason for Spark being Speedy than MapReduce?

Support for different language APIs like Scala, Java, Python and R

RDDs are immutable and fault-tolerant

**None of the above**

**The correct answer is "None of the above."**

**While both options mentioned are indeed features of Apache Spark, they are not the specific reasons for Spark being faster than MapReduce.**

**The primary reason for Spark's speed and performance advantage over MapReduce is its ability to perform in-memory processing. Unlike MapReduce, which relies heavily on disk I/O for data storage and retrieval, Spark leverages RDDs (Resilient Distributed Datasets) to cache data in memory and perform computations in-memory.**

**By keeping data in memory, Spark avoids the overhead of frequent disk read and write operations, leading to significantly faster processing times. Additionally, Spark optimizes the execution of operations by enabling data pipelining and lazy evaluation, minimizing data shuffling and improving overall efficiency.**

**While *Spark's support for different language APIs and the immutability and fault-tolerance of RDDs are important features that contribute to its versatility and reliability, they are NOT the main factors that make Spark faster than MapReduce*.**

**The correct answer is "None of the above." *The primary reason for Spark's speediness compared to MapReduce is its in-memory processing capability*.**

## 24.- Can you combine the libraries of Apache Spark into the same Application, for example, MLlib, GraphX, SQL and DataFrames etc.

**Yes**

No

**Yes, you can combine multiple libraries of Apache Spark into the same application. Spark is designed to be a unified analytics engine, providing various libraries and APIs that can be used together in a single application.**

**You can combine libraries such as MLlib (Machine Learning Library), GraphX (Graph Processing Library), Spark SQL, and DataFrames in the same Spark application. This allows you to perform a wide range of data processing, analytics, and machine learning tasks using a single framework.**

**You can use Spark SQL and DataFrames for querying and manipulating structured data, MLlib for training and evaluating machine learning models, and GraphX for performing graph computations and analysis. By combining these libraries, you can leverage the capabilities of Spark to process different types of data and perform diverse analytical tasks within the same application.**


## 25.- Which of the following is true for RDD?

RDD is programming paradigm

**RDD in Apache Spark is an immutable collection of objects**

It is database

None of the above


**The statement "RDD in Apache Spark is an immutable collection of objects" is true for RDD (Resilient Distributed Dataset).**

**An RDD is an immutable distributed collection of objects in Apache Spark. It represents a fundamental data abstraction in Spark and serves as the primary data structure for data processing tasks. RDDs are designed to be fault-tolerant and can be processed in parallel across a cluster of machines.**

**RDDs are immutable, meaning that once created, they cannot be modified. Instead, RDD transformations create new RDDs as the result of the transformation operations. This immutability ensures the fault-tolerance and consistency of data in distributed computations.**

**RDD is not a programming paradigm or a database; it is a fundamental data structure in Spark that provides resilience, immutability, and distributed processing capabilities.**


## 26.-  Which of the following is not a function of Spark Context in Apache Spark?

**Entry point to Spark SQL**

To Access various services

To set the configuration

To get the current status of Spark Application


**The function "Entry point to Spark SQL" is not a direct function of Spark Context in Apache Spark.**

***Spark Context is the main entry point and the foundation of the Spark application*. It provides the connection to the Spark cluster and coordinates the execution of tasks. However, *Spark Context itself does not directly serve as an entry point to Spark SQL*.**

**To use Spark SQL, you need to create a separate SQLContext or SparkSession, which provides a programming interface for working with structured and semi-structured data using SQL-like queries and DataFrame API. The SQLContext or SparkSession is typically created using the Spark Context as a base.**

**The functions of Spark Context include:**

***To access various services:* Spark Context provides access to various services and APIs within Spark, including Spark Streaming, Spark SQL, MLlib, GraphX, and more.**

***To set the configuration:* Spark Context allows you to set various configuration options for your Spark application, such as the number of executors, memory allocation, logging levels, and other runtime settings.**

***To get the current status of the Spark application:* Spark Context provides information about the current status of the Spark application, including job progress, stages, and task execution details.**

***The correct option is "Entry point to Spark SQL*." Spark Context serves as the entry point for Spark applications and provides functions related to accessing services, setting configurations, and getting the status of the application. However, Spark SQL functionality is accessed through a separate SQLContext or SparkSession object.**

**EXAMPLE**:

```scala
import org.apache.spark.SparkContext
import org.apache.spark.sql.{SparkSession, DataFrame}

// Creating a new SparkContext
val sc = new SparkContext("local[*]", "MySparkApplication")

// Creating a SparkSession using the SparkContext
val spark = SparkSession.builder().appName("MySparkApplication").getOrCreate()

// Reading data from a Parquet file into a DataFrame
val data_df: DataFrame = spark.read.parquet("path/to/parquet_file.parquet")

// Performing some operations on the DataFrame
val result: DataFrame = data_df.filter($"age" > 30).groupBy("gender").count()

// Displaying the result
result.show()

// Stopping the SparkContext
sc.stop()
```

**EJEMPLO:**

```scala
import org.apache.spark.SparkContext
import org.apache.spark.sql.{SparkSession, DataFrame}

// Creando un nuevo SparkContext
val sc = new SparkContext("local[*]", "MiAplicacionSpark")

// Creando un SparkSession usando el SparkContext
val spark = SparkSession.builder().appName("MiAplicacionSpark").getOrCreate()

// Leyendo datos desde un archivo Parquet en un DataFrame
val data_df: DataFrame = spark.read.parquet("ruta/al/archivo.parquet")

// Realizando algunas operaciones en el DataFrame
val resultado: DataFrame = data_df.filter($"edad" > 30).groupBy("genero").count()

// Mostrando el resultado
resultado.show()

// Deteniendo el SparkContext
sc.stop()
```

## 27.-  What are the features of Spark RDD?

In-memory computation

Lazy evaluations

Fault Tolerance

**All of the above**


**All of the above options are features of Spark RDD (Resilient Distributed Dataset).**

**In-memory computation: RDDs allow data to be stored in memory, enabling faster processing by reducing disk I/O operations.**

**Lazy evaluations: RDDs support lazy evaluations, meaning the transformations on RDDs are not executed immediately. Instead, the transformations are recorded and executed only when an action is performed, allowing for optimized execution plans.**

**Fault tolerance: RDDs are fault-tolerant by design. They track the lineage information, which allows them to recover lost data partitions in case of failures.**

**Therefore, all of these features make RDDs a powerful abstraction in Spark for distributed data processing.**

## 28.- How many Spark Context can be active per JVM?

More than one

**Only one**

Not specific

None of the above


**In Apache Spark, *only one SparkContext can be active per JVM*. Attempting to create multiple SparkContext instances in the same JVM can lead to conflicts and unexpected behavior. SparkContext is the entry point for interacting with Spark functionality, and it represents the connection to a Spark cluster. It manages the underlying resources and configurations necessary for Spark applications.**

## 29.- In how many ways RDD can be created?


4

3

**2**

1

**RDD (Resilient Distributed Dataset) can be created in two ways:**

**1. Parallelizing an existing collection: RDDs can be created by parallelizing an existing collection in the driver program. This involves distributing the collection across the cluster and creating partitions of data.**

**2. Loading external datasets: RDDs can also be created by loading data from external storage systems such as Hadoop Distributed File System (HDFS), local file systems, or any data source supported by Hadoop InputFormats.**

## 30.- How many tasks does Spark run on each partition?


Any number of task

**One**

More than one less than five


**Spark runs one task per partition by default. Each partition of an RDD is processed by a single task. However, it's important to note that the number of tasks can be configured and adjusted based on factors such as the cluster's configuration, available resources, and the specific operations being performed. By specifying the number of partitions or using operations like repartition() or coalesce(), the number of tasks can be increased or decreased. So, while the default is one task per partition, it is possible to have more than one task per partition depending on the setup and requirements of the Spark application.**

## 31.- Can we edit the data of RDD, for example, the case conversion?

Yes

No

**No, we cannot directly edit the data of an RDD in Apache Spark. RDDs are immutable, meaning they cannot be modified once created. However, we can apply transformations to RDDs to create new RDDs with desired modifications or operations.**

**Case conversion refers to changing the case (uppercase or lowercase) of the characters in a string. For example, converting "Hello" to "hello" or "WORLD" to "world".**

## 32.- Which of the following is not a transformation?

Flatmap

Map

**Reduce**

Filter


**Reduce is not a transformation in Apache Spark. It is an action.**

**Transformations in Spark are *operations that create a new RDD by applying a function to the elements of an existing RDD*. They are *lazy operations, meaning they are not executed immediately but recorded for future computations*. Examples of transformations include map, filter, and flatMap, which apply a function to each element of the RDD and generate a new RDD based on the transformation logic.**

**On the other hand, *actions in Spark trigger the execution of transformations and return a result or perform some action on the data*. Examples of actions include *reduce, collect, count*, which perform computations and return results or output the data.**

***The correct answer is Reduce.***

## 33.- Which of the following is not an action?

collect()

take(n)

top()

**map**

**Actions in Spark are *operations that trigger the execution of transformations and return a result to the driver program or perform some action on the data*. Examples of actions include collect(), which retrieves all elements of an RDD or DataFrame and returns them as an array, take(n), which retrieves the first n elements from an RDD or DataFrame, and top(), which returns the top n elements from an RDD or DataFrame based on a custom ordering.**

**On the other hand, *map is a transformation in Spark*. It a*pplies a function to each element of an RDD or DataFrame and returns a new RDD or DataFrame consisting of the transformed elements*.**

**The correct answer is map.**

## 34.- Does Spark R make use of MLlib in any aspect?

**Yes**

No


## 35.- You can connect R program to a Spark cluster from â€“

RStudio

R Shell

Rscript

**All of the above**

**You can connect an R program to a Spark cluster from RStudio, R Shell, and Rscript. Spark provides the sparklyr package, which allows R users to interact with Spark and leverage its capabilities from within the R environment.**

**You can establish a connection to a Spark cluster, perform data manipulations, run distributed computations, and execute machine learning algorithms using the sparklyr package. Whether you are working in RStudio, the R Shell, or running an R script using Rscript, you can utilize the sparklyr package to connect to a Spark cluster and leverage its distributed processing capabilities.**


## 36.-  For Multiclass classification problem which algorithm is not the solution?

Naive Bayes

Random Forests

**Logistic Regression**

Decision Trees

**The best answer would be "Naive Bayes." Naive Bayes is typically used for binary classification tasks or problems with discrete input features. It assumes that the features are conditionally independent, which may not hold in more complex multiclass scenarios. In contrast, Random Forests, Logistic Regression, and Decision Trees can all handle multiclass classification problems effectively.**

## 37.- For Regression problem which algorithm is not the solution?

**Logistic Regression**

Ridge Regression

Decision Trees

Gradient-Boosted Trees


## 38.- Which of the following is true about DataFrame?

DataFrames provide a more user-friendly API than RDDs.

DataFrame API have provision for compile-time type safety

**Both the above**

**Both the statements are true about DataFrame.**

**DataFrames provide a more user-friendly API than RDDs: DataFrames in Apache Spark provide a higher-level, tabular data structure with a more user-friendly API compared to RDDs (Resilient Distributed Datasets). The DataFrame API allows users to express complex data transformations and manipulations using a SQL-like syntax, making it easier to work with structured data. DataFrames offer a simplified programming model that abstracts away the low-level details of distributed computing.**

**DataFrame API has provision for compile-time type safety: In Spark 2.0 and later versions, DataFrame API and Dataset API have been unified, where DataFrame is a type alias for Dataset[Row]. This means that DataFrames benefit from the type safety and compile-time checks provided by Datasets. The Dataset API allows for compile-time type safety by leveraging the static typing features of Scala and Java. This ensures that type-related errors are caught at compile-time rather than at runtime, leading to more robust and reliable code.**

**DataFrames provide a more user-friendly and expressive API compared to RDDs and also offer compile-time type safety through the underlying Dataset API. These features make DataFrames a powerful tool for working with structured data in Apache Spark.**


## 39.- Which of the following is a tool of Machine Learning Library?

Persistence

Utilities like linear algebra, statistics

Pipelines

**All of the above**

**All of the above options are tools of the Machine Learning Library (MLlib) in Apache Spark.**

***Persistence:* MLlib provides persistence capabilities that allow you to save and load trained models, enabling you to reuse and deploy them in production environments.**

***Utilities like linear algebra, statistics:* MLlib includes various utilities for linear algebra operations, statistical calculations, and mathematical functions. These utilities provide the foundation for many machine learning algorithms and data preprocessing tasks.**

***Pipelines:* MLlib supports the concept of pipelines, which are workflows for building and deploying machine learning models. Pipelines enable you to chain together multiple data transformations and machine learning algorithms into a single workflow, making it easier to manage and reproduce the entire machine learning process.**

**MLlib is a comprehensive library in Apache Spark that offers a wide range of machine learning algorithms, utilities, and tools to support various aspects of the machine learning lifecycle. It provides a scalable and distributed framework for building and deploying machine learning applications on large datasets.**


## 40.- Is MLlib deprecated?

Yes

**No**

**Is MLlib deprecated?**

**No. MLlib includes both the RDD-based API and the DataFrame-based API. The RDD-based API is now in maintenance mode. But neither API is deprecated, nor MLlib as a whole.**

M**Llib continues to be actively developed and maintained as a core component of Apache Spark's machine learning capabilities.**

**Retrieved on June 2023 from:**
https://spark.apache.org/docs/latest/ml-guide.html

## 41.- Which of the following is false for Apache Spark?

It provides high-level API in Java, Python, R, Scala

It can be integrated with Hadoop and can process existing Hadoop HDFS data

**Spark is an open source framework which is written in Java**

Spark is 100 times faster than Bigdata Hadoop


**Apache Spark is primarily implemented in Scala, not Java. While Spark does provide APIs in multiple languages, including Java, Python, R, and Scala, its core implementation is primarily in Scala. Therefore, the statement "Spark is an open-source framework which is written in Java" is false. The correct statement is that Spark is an open-source framework written in Scala.** 


## 42.- Which of the following is true for Spark SQL?

It is the kernel of Spark

Provides an execution platform for all the Spark applications

**It enables users to run SQL / HQL queries on the top of Spark.**

Enables powerful interactive and data analytics application across live streaming data


**Spark SQL is a module in Apache Spark that provides a programming interface for querying structured and semi-structured data using SQL (Structured Query Language) or HiveQL (Hive Query Language). It allows users to write SQL-like queries to perform data manipulation, aggregation, filtering, and other operations on distributed datasets in Spark. Spark SQL seamlessly integrates with the Spark ecosystem and can be used alongside other Spark libraries and components.**

**While the other options mentioned may be associated with Spark and its capabilities, they are not specific to Spark SQL:**

**"It is the kernel of Spark" is not true. The kernel of Spark refers to the core computation engine of Spark, which is responsible for executing distributed data processing tasks.**

**"Provides an execution platform for all the Spark applications" is not entirely accurate. While Spark as a whole provides an execution platform for various types of applications, Spark SQL specifically focuses on SQL-based data querying and processing.**

**"Enables powerful interactive and data analytics application across live streaming data" is not specifically attributed to Spark SQL. While Spark Streaming is a component of Apache Spark that enables real-time data processing and analytics, it is not exclusive to Spark SQL.**

**The true statement for Spark SQL is *its ability to enable users to run SQL / HQL queries on top of Spark*.**

43.- Which of the following is true for Spark core?

**It is the kernel of Spark**

It enables users to run SQL / HQL queries on the top of Spark.

It is the scalable machine learning library which delivers efficiencies

Improves the performance of iterative algorithm drastically.


**Spark Core is the foundational component of Apache Spark, serving as the distributed computing engine and the heart of the Spark framework. It provides the basic functionality and infrastructure for distributed data processing, including task scheduling, memory management, fault tolerance, and communication between different nodes in a Spark cluster.**

**Spark Core is responsible for executing parallel data processing tasks on distributed datasets using RDD (Resilient Distributed Datasets) as the fundamental data structure.**

**Regarding the other options:**

**"It enables users to run SQL / HQL queries on the top of Spark" refers to Spark SQL, a module in Apache Spark specifically designed for SQL-based querying and processing of structured data.**

**"It is the scalable machine learning library which delivers efficiencies" refers to MLlib, the machine learning library in Spark that provides a wide range of scalable and distributed machine learning algorithms and utilities.**

**"Improves the performance of iterative algorithm drastically" is a characteristic of Spark's in-memory processing capabilities and optimized execution engine, which can significantly speed up iterative algorithms.**

**The true statement for Spark Core is that *it serves as the kernel of Spark*.**

## 44.- Which of the following is true for Spark R?

**It allows data scientists to analyze large datasets and interactively run jobs**

It is the kernel of Spark

It is the scalable machine learning library which delivers efficiencies

It enables users to run SQL / HQL queries on the top of Spark.


**The true statement for Spark R is that it allows data scientists to analyze large datasets and interactively run jobs using the R programming language. SparkR is an R package that provides an interface for running Spark jobs and performing data analysis on large datasets using the R programming language. It allows data scientists and analysts to leverage the distributed computing power of Apache Spark and perform interactive data analysis, data processing, and machine learning tasks using familiar R syntax and functions. SparkR enables seamless integration of R with Spark and provides access to various Spark features and functionalities.**

## 45.- Which of the following is true for Spark MLlib?

Provides an execution platform for all the Spark applications

**It is the scalable machine learning library which delivers efficiencies**

enables powerful interactive and data analytics application across live streaming data

All of the above

**Spark MLlib is the machine learning library in Apache Spark that provides a wide range of scalable and distributed machine learning algorithms and utilities. It offers a high-level API for building machine learning pipelines and supports various tasks, including classification, regression, clustering, collaborative filtering, and more. MLlib leverages the distributed computing capabilities of Spark to process large datasets in parallel, delivering efficient and scalable machine learning solutions.**

## 46.- Which of the following is true for Spark Shell?


It helps Spark applications to easily run on the command line of the system

It runs/tests application code interactively

It allows reading from many types of data sources

**All of the above**

**Spark Shell is an interactive command-line interface provided by Apache Spark, and all the mentioned options are true for it. Here's a breakdown of each option:**

**"It helps Spark applications to easily run on the command line of the system": Spark Shell allows you to run Spark applications directly from the command line without the need for compiling and packaging the code into a separate JAR file. It provides a convenient way to interactively work with Spark without setting up a separate development environment.**

**"It runs/tests application code interactively": Spark Shell enables you to write and execute Spark code interactively, making it useful for running and testing application code snippets on-the-fly. It provides a Read-Eval-Print Loop (REPL) environment where you can write Spark code, execute it, and immediately see the results.**

**"It allows reading from many types of data sources": Spark Shell supports reading data from various data sources such as text files, CSV files, JSON files, Parquet files, Hive tables, and more. It provides built-in APIs and connectors to access and process data from different storage systems and formats.**



## 47.- Which of the following is true for RDD?

We can operate Spark RDDs in parallel with a low-level API

RDDs are similar to the table in a relational database

It allows processing of a large amount of structured data

It has built-in optimization engine

**RDD (Resilient Distributed Dataset) in Apache Spark allows parallel processing of data through a low-level API. RDDs provide a distributed collection of objects that can be processed in parallel across a cluster of machines. Users can perform operations on RDDs using functional transformations like map, filter, and reduce, allowing for efficient and parallel computation.**

**Regarding the other options:**

**"RDDs are similar to the table in a relational database" is not an accurate description of RDDs. RDDs are a fundamental data structure in Spark and do not have a direct analogy to relational database tables.**

**"It allows processing of a large amount of structured data" is a generic statement that applies to Spark in general but is not specific to RDDs.**

**"It has a built-in optimization engine" is not true for RDDs. While Spark itself has a built-in optimization engine that optimizes the execution of Spark jobs, RDDs do not have their own separate optimization engine.**

**The true statement for RDD is that *we can operate Spark RDDs in parallel with a low-level API*.**

## 48.- RDD are fault-tolerant and immutable

**True**

False

**RDDs (Resilient Distributed Datasets) in Apache Spark are designed to be fault-tolerant and immutable. Fault-tolerance means that RDDs can recover from failures automatically, ensuring the reliability of data processing. If a node in the cluster fails, RDDs can be reconstructed from lineage information, allowing the computation to continue without data loss.**

**RDDs are immutable, which means that once created, their contents cannot be modified. Instead of updating data in place, RDD transformations create new RDDs as the result. This immutability ensures data consistency and simplifies parallel processing, as RDDs can be processed in a distributed and concurrent manner without concerns of data modification.**

## 49.- In which of the following cases do we keep the data in-memory?

Iterative algorithms

Interactive data mining tools

**Both the above**

**In Apache Spark, both iterative algorithms and interactive data mining tools can benefit from keeping the data in-memory.**

**Iterative Algorithms: Many machine learning and optimization algorithms involve iterative computations, where the same dataset is processed multiple times with updates in each iteration. By keeping the data in-memory, Spark avoids the overhead of reading and writing data from and to disk, leading to significant performance improvements for iterative algorithms.**

**Interactive Data Mining Tools: Interactive data mining tools require fast response times for queries and exploratory analysis. By keeping the data in-memory, Spark enables quick access and manipulation of the data, allowing users to interactively explore and analyze large datasets in real-time.**

**By leveraging Spark's in-memory processing capabilities, both iterative algorithms and interactive data mining tools can achieve significant performance gains, making them suitable use cases for keeping the data in-memory.**

50.- When does Apache Spark evaluate RDD?

ddddd

Upon transformation

**On both transformation and action**

**Apache Spark evaluates RDD (Resilient Distributed Dataset) upon action.**

**In Spark, RDDs are lazily evaluated, meaning that transformations on RDDs are not immediately executed. Instead, Spark builds a logical execution plan, known as the DAG (Directed Acyclic Graph), which represents the sequence of transformations to be applied on the data. The transformations are recorded and executed only when an action is called on the RDD.**

**Actions in Spark trigger the evaluation of the RDD lineage and initiate the execution of the transformations in the DAG. The result of the action is returned to the driver program or can be written to an external storage system.**

**RDDs are evaluated on both transformation and action, but the actual execution and computation happen when an action is encountered.**


## 51.- The read operation on RDD is

**Fine-grained**

Coarse-grained

Either fine-grained or coarse-grained

Neither fine-grained nor coarse-grained

**The read operation on an RDD (Resilient Distributed Dataset) in the context of Apache Spark is fine-grained. RDDs in Spark are a fundamental data structure that allows for distributed data processing and fault tolerance. *Fine-grained operations refer to the ability to perform transformations and actions on individual elements within an RDD*. These operations are performed in parallel across the distributed nodes in a cluster, enabling efficient and scalable data processing.**


## 52.- The write operation on RDD is

Fine-grained

Coarse-grained

**Either fine-grained or coarse-grained**

Neither fine-grained nor coarse-grained

**The write operation on an RDD (Resilient Distributed Dataset) in Apache Spark can be either fine-grained or coarse-grained, depending on the specific operation being performed.**

**Fine-grained write operations involve updating individual elements within an RDD. This means that only specific elements or partitions of the RDD are modified while keeping the rest of the data intact. Examples of fine-grained write operations include updating values within an RDD or appending new data to it without affecting the existing elements.**

**Coarse-grained write operations, on the other hand, involve modifying the entire RDD or a significant portion of it. This typically requires creating a new RDD with the updated data. Examples of coarse-grained write operations include replacing the entire RDD with a new dataset or merging multiple RDDs together.**

**The write operation on an RDD can be *either fine-grained or coarse-grained*, depending on the level of granularity required for the specific task at hand.**

## 53.- Is it possible to mitigate stragglers in RDD?

**Yes**
No

**It is possible to mitigate stragglers in RDD (Resilient Distributed Dataset) processing in Apache Spark. *Stragglers are the nodes or tasks in a distributed system that take significantly longer to complete their processing compared to other nodes, causing delays in overall job execution*.**

**There are several techniques and strategies that can be used to mitigate stragglers in RDD processing:**

***Speculative Execution:* Spark can launch multiple instances of a task on different nodes and use the result from the first completed instance, while cancelling the other instances once a result is obtained. This helps to mitigate the impact of stragglers by effectively executing the task in parallel on multiple nodes.**

***Task Prefetching:* Spark can preemptively fetch data required for a task to the nodes where the task is scheduled to execute. By fetching data in advance, the impact of stragglers that experience slow data fetching can be minimized.**

***Dynamic Resource Allocation:* Spark's dynamic resource allocation feature allows for automatic scaling of cluster resources based on the workload. By dynamically adjusting the allocation of resources, Spark can redistribute tasks from straggler nodes to other nodes with available resources, reducing the impact of stragglers on job execution.**

***Task Scheduling and Load Balancing:* Spark's task scheduling and load balancing mechanisms aim to evenly distribute tasks across the cluster. By avoiding data skew and ensuring balanced task assignment, the likelihood of stragglers can be reduced.**

***Data Partitioning:* Proper partitioning of data can help distribute the workload evenly across the cluster and minimize the impact of stragglers. Partitioning techniques such as range partitioning, hash partitioning, or custom partitioning can be employed to achieve better load balancing and performance.**

**By utilizing these techniques and strategies, Spark can effectively mitigate the impact of stragglers in RDD processing, improving overall job performance and reducing delays.**


## 54.- Fault Tolerance in RDD is achieved using

**Immutable nature of RDD**

DAG (Directed Acyclic Graph)

Lazy-evaluation

None of the above


**Fault tolerance in RDD (Resilient Distributed Dataset) is achieved using the immutable nature of RDDs.**

**RDDs in Apache Spark are designed to be immutable, meaning that once an RDD is created, its contents cannot be modified. Instead, any transformation applied to an RDD generates a new RDD as a result. This immutability plays a key role in achieving fault tolerance.**

**When a node in a cluster fails during RDD processing, the lost partitions of the RDD can be recomputed from the original dataset and lineage information. Lineage information is a record of the operations applied to an RDD, which allows Spark to reconstruct the RDD's state in case of failure. Since RDDs are immutable, Spark can rebuild the lost partitions by reapplying the transformations starting from the original data and following the lineage.**

**In this way, *the immutable nature of RDDs provides the foundation for fault tolerance in Spark. It enables the recovery of lost data partitions and ensures that the computation can continue seamlessly, even in the presence of failures*.**

***DAG (Directed Acyclic Graph) and lazy evaluation are not directly related to fault tolerance in RDDs. DAG represents the logical execution plan of transformations on RDDs, while lazy evaluation is a strategy where Spark postpones the execution of transformations until an action is called*. Although they are important concepts in Spark's processing model, they are not specifically responsible for achieving fault tolerance.**


## 55.- What is a transformation in Spark RDD?

**Takes RDD as input and produces one or more RDD as output.**

Returns final result of RDD computations.

The ways to send result from executors to the driver

None of the above

A transformation in Spark RDD (Resilient Distributed Dataset) is an operation that takes an RDD as input and produces one or more RDDs as output. Transformations are the core building blocks of Spark's data processing model. They create a new RDD by applying a specific operation or transformation to the elements of the input RDD.

Transformations are lazily evaluated, which means that they are not executed immediately when called. Instead, they build a directed acyclic graph (DAG) representing the series of transformations to be applied to the RDD. The actual execution of transformations occurs when an action is called on the RDD, triggering the evaluation of the DAG.

Examples of transformations in Spark RDD include map, filter, flatMap, reduceByKey, join, and many more. These transformations enable various data manipulations, aggregations, and operations to be performed on RDDs.

A transformation in Spark RDD takes an RDD as input and generates one or more RDDs as output, representing the intermediate steps of data processing before a final result is obtained.

## 56.-  What is action in Spark RDD?

**The ways to send result from executors to the driver**

Takes RDD as input and produces one or more RDD as output.

Creates one or many new RDDs

**All of the above


**Actions in Spark RDD trigger the execution of transformations, create a final result or value,and can save or send the result from the executors to the driver program.**

**An action in Spark RDD (Resilient Distributed Dataset) is *an operation that triggers the execution of the transformations defined on an RDD and returns a final result or value to the driver program. Actions are the operations that produce non-RDD results and initiate the actual computation on the RDD*.**

***Unlike transformations, which are lazily evaluated, actions are eagerly executed. When an action is called on an RDD, Spark evaluates the entire lineage of transformations and computes the final result. The result of an action can be a value, a collection of values, or saved to an external storage system*.**

**Examples of actions in Spark RDD include count, collect, reduce, first, take, save, and foreach, among others. These actions perform various operations such as counting the number of elements, collecting all elements to the driver program, aggregating values, retrieving the first element, taking a specific number of elements, saving the RDD to a file, or applying a function to each element.**




## 57.- Which of the following is true about narrow transformation â€“

The data required to compute resides on multiple partitions.

**The data required to compute resides on the single partition.**

Both the above

**Narrow transformations in Spark RDD are operations where the data required to compute the output of the transformation resides within a single partition of the RDD. These transformations do not require shuffling or data exchange between partitions. The computation can be performed locally on each partition without needing to access data from other partitions.**

**Examples of narrow transformations include map, filter, union, and other operations that can be applied independently to each partition of the RDD. These transformations are typically fast and efficient since they operate locally on each partition.**

**On the other hand, wide transformations are operations where the data required to compute the output is distributed across multiple partitions of the RDD. These transformations involve shuffling or data exchange between partitions. Examples of wide transformations include groupByKey, reduceByKey, join, and sortByKey, which require data from multiple partitions to be combined or reshuffled to produce the output.**

## 58.- Which of the following is true about wide transformation â€“

**The data required to compute resides on multiple partitions.**

The data required to compute resides on the single partition.

None of the both

## 59.- When we want to work with the actual dataset, at that point we use Transformation?

True

**False**

When we want to work with the actual dataset, we use actions, not transformations. Actions are operations in Spark that trigger the execution of the transformations and return a result or value to the driver program. Actions are responsible for initiating the computation on the RDD and producing a concrete result or side effect.

Transformations, on the other hand, are operations that define a new RDD based on existing RDDs. They are lazy in nature, meaning they do not execute immediately but build a DAG (Directed Acyclic Graph) that represents the computation to be performed. Transformations are applied to RDDs to create a logical plan for data processing.

To obtain the actual dataset or result of the computation, actions need to be called on the RDD. Actions cause the evaluation of the transformations and trigger the execution of the computation.

## 60.- The shortcomings of Hadoop MapReduce was overcome by Spark RDD by

Lazy-evaluation

DAG

In-memory processing

**All of the above**

**Spark RDD (Resilient Distributed Dataset) overcomes the shortcomings of Hadoop MapReduce by incorporating various features, including lazy evaluation, DAG (Directed Acyclic Graph), and in-memory processing.**

**Lazy evaluation: Spark RDD employs lazy evaluation, meaning that transformations on RDDs are not executed immediately. Instead, they are recorded as a sequence of operations in a DAG, allowing for efficient optimization and execution planning.**

**DAG: Spark RDD uses a DAG (Directed Acyclic Graph) to represent the logical execution plan of transformations on RDDs. The DAG allows Spark to optimize and schedule the execution of operations, leading to improved performance and resource utilization.**

**In-memory processing: Unlike Hadoop MapReduce, which relies heavily on disk-based storage and intermediate data I/O, Spark RDD leverages in-memory processing. By persisting data in memory across multiple operations and computations, Spark significantly reduces disk I/O and achieves faster data processing speeds.**

**These features collectively enable Spark RDD to overcome the limitations of Hadoop MapReduce, providing enhanced performance, flexibility, and fault tolerance for big data processing tasks.**

## 61.- What does Spark Engine do?

Scheduling

Distributing data across a cluster

Monitoring data across a cluster

**All of the above**

**The Spark engine is responsible for performing various tasks within the Apache Spark framework, including scheduling, distributing data across a cluster, and monitoring data across a cluster.**

**Scheduling: The Spark engine includes a scheduler that assigns tasks to different nodes in a cluster based on availability and resource allocation. It manages the execution of tasks and ensures efficient utilization of cluster resources.**

**Distributing data across a cluster: The Spark engine handles the distribution of data across multiple nodes in a cluster. It partitions and distributes RDDs (Resilient Distributed Datasets) or DataFrames across the cluster to enable parallel processing and efficient data computation.**

**Monitoring data across a cluster: The Spark engine provides monitoring and management capabilities to track the progress and status of data processing tasks across the cluster. It collects information about the execution, resource usage, and performance of the Spark application, allowing for effective monitoring and troubleshooting.**


## 62.- Caching is optimizing the technique

**True**

False


**Caching is an optimization technique used in various computing systems, including Apache Spark, to improve performance by storing frequently accessed data in memory. In Spark, caching allows RDDs (Resilient Distributed Datasets) or DataFrames to be persisted in memory across multiple operations and computations. This eliminates the need to recompute or retrieve the data from disk, resulting in faster access and reduced processing time.**

**By caching RDDs or DataFrames, Spark can avoid unnecessary re-computation and improve the efficiency of subsequent operations that depend on the cached data. Caching is particularly beneficial when there are iterative or repetitive computations performed on the same dataset.**

**Caching is *used to optimize performance by reducing data access latency and improving overall execution speed in Spark*.**

## 63.- Which of the following is the entry point of Spark Application â€“

**SparkSession**

SparkContext

None of the both

**SparkSession is the primary entry point for Spark functionality and replaces the older entry point, SparkContext. It provides a unified interface for interacting with Spark and enables you to work with various Spark features, such as DataFrame API, Dataset API, Spark SQL, and Spark Streaming.**

**With SparkSession, you can configure Spark application settings, create DataFrames and Datasets, execute SQL queries, and perform various operations on Spark data structures.**

## 64.- SparkContext guides how to access the Spark cluster.

**True**

False

**SparkContext is the entry point for accessing the Spark cluster. It represents the connection to a Spark cluster and provides the necessary APIs to interact with the cluster. It allows you to configure various cluster-level settings, create RDDs (Resilient Distributed Datasets), and perform distributed computations on the cluster.**

**SparkContext is responsible for coordinating the execution of tasks across the cluster and managing the resources. It provides methods for creating RDDs from data sources, performing transformations and actions on RDDs, and controlling the execution behavior of Spark applications.**


## 65.- Which of the following is the entry point of Spark SQL?

**SparkSession**

SparkContext

**The entry point of Spark SQL is the SparkSession.**

**SparkSession is the primary entry point for Spark SQL. It is responsible for enabling the execution of SQL queries and provides a programming interface to work with structured and semi-structured data using SQL, DataFrames, and Datasets.**

**SparkSession encapsulates the functionality of Spark SQL and integrates it with other components of Apache Spark, such as Spark Core, Spark Streaming, and MLlib. It allows you to create and manipulate DataFrames and Datasets, register temporary tables and views, execute SQL queries, and perform various data manipulations and transformations.**



## 66.- Which of the following is open-source?

Apache Spark

Apache Hadoop

Apache Flink

**All of the above**

**Apache Spark, Apache Hadoop, and Apache Flink, are open-source projects. Apache is an open-source software foundation that supports the development of various software projects. These 3 projects are designed for big data processing and analytics, offering features such as distributed storage, distributed processing, stream processing, and batch processing. They have gained popularity for their scalability, fault-tolerance, and wide range of data processing capabilities.**

## 67.- Apache Spark supports â€“

Batch processing

Stream processing

Graph processing

**All of the above**

**Apache Spark supports batch processing, stream processing, and graph processing. Apache Spark is a versatile and powerful distributed computing framework that provides APIs and libraries for processing large-scale data in various scenarios. It allows users to perform batch computations on large datasets, process real-time streaming data, and perform graph-based computations for graph analytics tasks. This flexibility makes Spark a popular choice for a wide range of data processing and analytics applications.**

## 68.- Which of the following is not true for map() Operation?

Map transforms an RDD of length N into another RDD of length N.

In the Map operation developer can define his own custom business logic.

It applies to each element of RDD and it returns the result as new RDD

**Map allows returning 0, 1 or more elements from map function.**

**In Spark's map() operation, each element of the RDD is transformed using the provided function, and *the result is a new RDD with the same number of elements as the original RDD*. The map() operation *applies the function to each element and produces a one-to-one mapping. It does not allow returning zero or more elements from the map function*.**

## 69.- FlatMap transforms an RDD of length N into another RDD of length M. which of the following is true for 
## N and M.   a) N>M   b) N<M   c) N<=M

Either a or b

Either b or c

**Either a or c**

**In the case of flatMap transformation, the resulting RDD can have a different length than the original RDD. It depends on the logic implemented in the flatMap function. It is possible for the resulting RDD to have more elements (N > M), or it could have fewer elements (N < M), or in some cases, it could have the same number of elements (N = M). Therefore, either option a) or c) could be true for the relationship between N and M.**

## 70.- Which of the following is a transformation?

take(n)

top()

countByValue()

**mapPartitionWithIndex()**

**Transformations in Apache Spark are operations that produce a new RDD or DataFrame from an existing one. They are lazily evaluated, meaning the execution is deferred until an action is called.**

**In the case of mapPartitionWithIndex(), it applies a function to each partition of the RDD while providing the index of each partition as an additional parameter to the function. It returns a new RDD resulting from the transformation.**

## 71.- Which of the following is action?

Union(dataset)

Intersection(other-dataset)

Distinct()

**CountByValue()**

**CountByValue() is an action. Actions in Apache Spark are operations that trigger the execution of transformations and return a result to the driver program or write data to an external storage system. Unlike transformations, actions cause the evaluation of the RDD or DataFrame and produce a result or a side effect.**

**In the case of CountByValue(), it counts the occurrences of each unique value in the RDD or DataFrame and returns the result as a map of value to count.**

## 72.- In aggregate function can we get the data type different from as that input data type?

**Yes**

No

**In the aggregate function of Apache Spark, it is possible to get a result with a different data type than the input data type.**

**The aggregate function allows you to perform a customizable aggregation on the elements of an RDD or DataFrame. It takes two functions as arguments: the first function combines the elements within a partition, and the second function combines the results from different partitions. The second function, often referred to as the "result function," can transform the aggregated result into a different data type if needed.**

**This flexibility in the aggregate function allows you to perform complex aggregations and produce results of different data types as per your requirement.**

## 73.- In which of the following Action the result is not returned to the driver.

collect()

top()

countByValue()

**foreach()**

**The foreach() action applies a function to each element of an RDD or DataFrame without returning any result to the driver program. It is commonly used for performing side effects such as writing the elements to an external system or updating shared variables.**

**On the other hand, collect(), top(), and countByValue() actions return results to the driver program. collect() returns all the elements of the RDD or DataFrame as an array, top() returns the top k elements based on a predefined ordering, and countByValue() returns a map with each unique element of the RDD or DataFrame and its count.**

## 74.- Which of the following is true for stateless transformation?

Uses data or intermediate results from previous batches and computes the result of the current batch.

Windowed operations and updateStateByKey() are two type of Stateless transformation.

**The processing of each batch has no dependency on the data of previous batches.**

None of the above

**Stateless transformations in Apache Spark do not use data or intermediate results from previous batches to compute the result of the current batch. Each batch is processed independently, without any dependency on the data processed in previous batches. Examples of stateless transformations include map(), filter(), and flatMap(), where the output for each input element is solely determined by the input element itself.**

**On the other hand, windowed operations and updateStateByKey() are examples of stateful transformations, where the processing of each batch considers the data or intermediate results from previous batches.**


## 75.- Which of the following is true for stateful transformation?

The processing of each batch has no dependency on the data of previous batches.

**Uses data or intermediate results from previous batches and computes the result of the current batch.**

Stateful transformations are simple RDD transformations.

None of the above

**Stateful transformations in Apache Spark utilize data or intermediate results from previous batches to compute the result of the current batch. These transformations maintain some form of state or context across multiple batches to perform computations that require information from previous batches. Examples of stateful transformations include windowed operations, such as reduceByKeyAndWindow() and updateStateByKey(), where the processing of each batch considers the data processed in previous batches.**

## 76.- The primary Machine Learning API for Spark is now the _____ based API

**DataFrame**

Dataset

RDD

All of the above


Apache Spark provides multiple APIs for machine learning, including RDD-based API, DataFrame-based API, and Dataset-based API. However, the DataFrame-based API has become the primary API for machine learning in Spark, offering a more user-friendly and efficient way to perform machine learning tasks. It leverages the structured data processing capabilities of DataFrames and benefits from the optimizations provided by Spark's Catalyst query optimizer.

## 77.- Which of the following is a module for Structured data processing?

GraphX

MLlib

**Spark SQL**

Spark R

**Spark SQL provides a programming interface for querying structured and semi-structured data using SQL, as well as a DataFrame API for working with structured data in a more programmatic way. It allows users to execute SQL queries, access various data sources, and perform data manipulation operations on structured data. Spark SQL integrates with other Spark modules like MLlib and GraphX, enabling seamless integration of structured data processing with machine learning and graph analytics.**

## 78.- SparkSQL translates commands into codes. These codes are processed by

Driver nodes

**Executor Nodes**

Cluster manager

None of the above

**SparkSQL translates commands into codes that are processed by Executor Nodes. In Spark, the Driver program is responsible for parsing and optimizing the SparkSQL commands and generating an execution plan. *The generated execution plan is then sent to the Executor Nodes, which are responsible for executing the tasks in parallel on the cluster. Each Executor Node processes a subset of the data and performs the required operations as per the execution plan. The results are then combined and returned to the Driver program for further processing or output*. The Cluster Manager, on the other hand, is responsible for managing the allocation of resources and scheduling tasks across the cluster, but it does not directly process the SQL commands.**

## 79.- Spark SQL plays the main role in the optimization of queries.

**True**

False

**Spark SQL plays a crucial role in the optimization of queries. It includes an advanced optimizer that analyzes the query plans, applies various optimization techniques, and generates an optimized execution plan. This optimization process helps to improve the performance of SQL queries by minimizing the amount of data shuffled, reducing the number of operations, and leveraging various optimization strategies. By optimizing the queries, Spark SQL aims to execute them efficiently and provide faster query processing times.**

## 80.- This optimizer is based on functional programming construct in

Java

**Scala**

Python

R

**The optimizer in Spark SQL is primarily based on functional programming constructs in Scala. Scala is the programming language in which Apache Spark is implemented, and it offers powerful functional programming capabilities. Spark SQL leverages these constructs to perform query optimization, such as applying predicate pushdown, join reordering, and expression simplification. The functional programming paradigm in Scala allows for concise and expressive code, which contributes to the efficiency and effectiveness of the optimization process in Spark SQL.**

## 81.- Catalyst Optimizer supports either rule-based or cost-based optimization.

**True**

False

**The Catalyst Optimizer in Apache Spark supports both rule-based and cost-based optimization techniques. The rule-based optimization involves applying a set of predefined rules to transform and optimize query plans. These rules are designed to perform common query optimizations such as predicate pushdown, projection pruning, and join reordering. On the other hand, the cost-based optimization uses statistical information about the data and execution costs to estimate the most efficient query plan. By considering factors like data distribution, data size, and resource availability, the cost-based optimizer can make more informed decisions to optimize query execution. The combination of rule-based and cost-based optimizations in the Catalyst Optimizer helps Spark generate efficient query plans and improve overall performance.**

## 82.- Which of the following is not true for Catalyst Optimizer?

Catalyst optimizer makes use of pattern matching feature.

Catalyst contains the tree and the set of rulesto manipulate the tree.

**There are no specific libraries to process relational queries.**

There are different rule sets which handle different phases of query.

**The statement "There are no specific libraries to process relational queries" is not true for Catalyst Optimizer. Catalyst includes specific libraries and components that are dedicated to processing relational queries. It provides a comprehensive framework for analyzing, optimizing, and executing relational queries in Spark SQL. The Catalyst Optimizer employs various techniques, such as rule-based and cost-based optimizations, to transform and optimize query plans for efficient execution. It leverages pattern matching capabilities to match and apply transformation rules to the query tree. Additionally, Catalyst incorporates different rule sets that handle different phases of query optimization, allowing for modular and customizable optimizations.**


## 83.- Which of the following is true for the tree in Catalyst optimizer?

A tree is the main data type in the catalyst.

New nodes are defined as subclasses of TreeNode class.

A tree contains a node object.

**All of the above**

**All of the statements are true for the tree in Catalyst optimizer.**

**A tree is indeed the main data type in Catalyst optimizer, representing the logical and physical plans of queries. It provides a hierarchical structure to represent the query execution plan.**

**New nodes in the tree are defined as subclasses of the TreeNode class. Catalyst uses a rich set of node types to represent various operations and expressions in the query plan. Each node subclass encapsulates specific functionality and properties.**

**A tree in Catalyst optimizer consists of multiple nodes, forming a hierarchical structure. Each node represents an operation or expression in the query plan, and the tree structure defines the relationships and dependencies between these nodes.**


## 84.- Which of the following is true for the rule in Catalyst optimizer?

We can manipulate tree using rules.

We can define rules as a function from one tree to another tree.

Using rule we get the pattern that matches each pattern to a result.

**All of the above**

**All of the statements are true for the rules in Catalyst optimizer.**

**Rules play a crucial role in the optimization process of Catalyst optimizer. They are used to manipulate the query plan tree by applying transformations and optimizations.**

**We can indeed manipulate the tree using rules. Each rule is designed to perform a specific transformation or optimization on the query plan tree.**

**Rules in Catalyst optimizer are defined as functions from one tree to another tree. They take a specific pattern or structure in the tree as input and apply the necessary transformations to produce a new tree.**

**Using rules, we can match patterns in the query plan tree and apply the corresponding transformations. By defining rules for different patterns, Catalyst optimizer can systematically optimize the query plan by replacing sub-trees with more efficient alternatives.**

## 85.- Which of the following is not a Spark SQL query execution phases? ***

Analysis

Logical Optimization

**Execution**

Physical planning


## 86.- In Spark SQL optimization which of the following is not present in the logical plan â€“

Constant folding

**Abstract syntax tree**

Projection pruning

Predicate pushdown



**Abstract syntax tree is not present in the logical plan of Spark SQL optimization.**


## 87.- In the analysis phase which is the correct order of execution after forming unresolved logical plan.     ***  
                             
                             a. Search relation BY NAME FROM CATALOG. 

                             b.Determine which attributes match to the same value to give them unique ID.     

                             c. Map the name attribute     

                             d. Propagate and push type through expressions


abcd

**acbd**

adbc

dcab



## 88.- The Physical planning phase of Query optimization we can use both Coast-based and Rule-based optimization. ***

True

**False**

**In the Physical planning phase of query optimization in Spark SQL, only rule-based optimization is used. Cost-based optimization is not utilized in the physical planning phase.**


## 89.- DataFrame in Apache Spark prevails over RDD and does not contain any feature of RDD.

True

**False**

**DataFrames in Apache Spark are built on top of RDDs (Resilient Distributed Datasets) and provide a higher-level API for structured data processing. DataFrames retain many features of RDDs, such as distributed computing and fault tolerance. DataFrames offer additional optimizations and optimizations for structured data processing, such as schema enforcement, query optimization, and Catalyst optimizer. Therefore, DataFrames in Apache Spark build upon and enhance the capabilities of RDDs.**


## 90.- Which of the following are the common feature of RDD and DataFrame?

Immutability

In-memory

Resilient

**All of the above**

**(Immutability, In-memory, Resilient) are common to both RDD (Resilient Distributed Dataset) and DataFrame in Apache Spark.**


## 91.- Which of the following is not true for DataFrame?

**DataFrame in Apache Spark is behind RDD**

We can build DataFrame from different data sources. structured data file, tables in Hive

The Application Programming Interface (APIs) of DataFrame is available in various languages

Both in Scala and Java, we represent DataFrame as Dataset of rows.

**The statement "DataFrame in Apache Spark is behind RDD" is not true. DataFrames are a higher-level abstraction built on top of RDDs, providing a more user-friendly API for structured data processing.**


## 92.- In Dataframe in Spark Once the domain object is converted into a data frame, the regeneration of domain object is not possible.

**True**

False

**Once a domain object is converted into a DataFrame in Spark, it loses its original structure and is transformed into a tabular format. This transformation is irreversible, and the original domain object cannot be regenerated directly from the DataFrame.**

## 93.- DataFrame API has provision for compile-time type safety.

True

**False**

**The DataFrame API in Spark does not provide compile-time type safety. Unlike strongly typed programming languages like Scala and Java, the DataFrame API in Spark is dynamically typed. It operates on a schema-aware structure, but the type safety is not enforced at compile-time.**


## 94.- We can create DataFrame using:

Tables in Hive

Structured data files

External databases

**All of the above**

**All of the above options can be used to create a DataFrame in Spark. DataFrame can be created from various data sources such as tables in Hive, structured data files (e.g., CSV, JSON, Parquet), and external databases (e.g., MySQL, PostgreSQL) using the appropriate APIs and connectors provided by Spark.**


## 95.- Which of the following is the fundamental data structure of Spark

**RDD**

DataFrame

Dataset

None of the above


**The fundamental data structure of Spark is RDD (Resilient Distributed Dataset). RDDs are immutable distributed collections of objects that can be processed in parallel across a cluster. They provide fault tolerance and are the building blocks of data processing in Spark. While DataFrame and Dataset are higher-level abstractions introduced in Spark to provide more structured and optimized data processing capabilities, RDDs remain a fundamental and versatile data structure in Spark.**


## 96.-  Which of the following provide the object-oriented programming interface

RDD

DataFrame

**Dataset**

None of the above

**The object-oriented programming interface in Apache Spark is provided by the Dataset API. Datasets are a higher-level abstraction introduced in Spark that provide a strongly-typed, object-oriented programming interface. Datasets are built on top of RDDs and combine the benefits of RDDs (such as distributed processing and fault tolerance) with the benefits of structured data processing (such as compile-time type safety and optimized execution). DataFrame, on the other hand, is a distributed collection of data organized into named columns and provides a more relational query-like interface. RDDs also support an object-oriented programming style, but they do not have the same level of type safety and optimization features as Datasets.**


## 97.- After transforming into DataFrame one cannot regenerate a domain object

**True**

False

**Once data is transformed into a DataFrame in Apache Spark, it is represented as a structured collection of data organized into named columns. The DataFrame is an abstraction that provides a high-level interface for manipulating and querying data. However, the DataFrame does not retain the original domain object structure or semantics. It is a schema-based representation that allows efficient processing and optimization of data operations. Therefore, once the data is transformed into a DataFrame, it cannot be easily regenerated into the original domain object without additional transformations or conversions.**


## 98.- RDD allows Java serialization

**True**

False

**True. RDD (Resilient Distributed Dataset) in Apache Spark allows Java serialization. Java serialization is one of the default serialization formats supported by Spark for serializing the data within RDDs. However, Spark also provides support for other serialization formats like Kryo, which is known for its faster serialization and smaller object size. Nonetheless, Java serialization is still a valid option when working with RDDs in Spark.**

## 99.- Which of the following make use of an encoder for serialization.

RDD

DataFrame

**Dataset**

None of the above

**The correct answer is Dataset. Datasets in Apache Spark make use of an encoder for serialization. Encoders are a feature of Datasets that allow for efficient serialization and deserialization of data. They provide a way to convert between JVM objects and Spark internal binary format, enabling better performance compared to traditional Java serialization or Kryo serialization used by RDDs. DataFrames, on the other hand, do not use encoders directly, but Datasets can be easily converted to DataFrames if needed.**


## 100.- Apache Spark is presently added in all major distribution of Hadoop

**True**

False

**True. Apache Spark is commonly included in major distributions of Hadoop, such as Cloudera, Hortonworks, and MapR. Spark can be seamlessly integrated with Hadoop ecosystems, leveraging Hadoop's distributed file system (HDFS) and resource management frameworks like YARN or Apache Mesos. This allows Spark to take advantage of the scalability, fault tolerance, and data processing capabilities of Hadoop.**

## 101.- Does Dataset API support Python and R.

Yes

**No**

**No. The Dataset API in Apache Spark is primarily designed for use with Scala and Java programming languages. While Spark supports Python and R through the PySpark and SparkR APIs, the Dataset API is not available directly in Python or R. Instead, Python users can work with DataFrames, which provide similar functionality to Datasets, while R users can work with SparkR DataFrames.**


## 102.- Which of the following is slow to perform simple grouping and aggregation operations.

**RDD**

DataFrame

Dataset

All of the above

**RDD (Resilient Distributed Dataset) in Apache Spark is a low-level data structure that provides resilience and fault tolerance but lacks built-in optimization for querying and aggregation operations. RDDs require more manual coding and are generally slower for performing simple grouping and aggregation operations compared to DataFrames and Datasets, which offer higher-level abstractions and optimization techniques. DataFrame and Dataset APIs provide more efficient execution plans and optimizations for common data manipulation tasks like grouping and aggregation.**


## 103.- Which of the following is good for low-level transformation and actions.

**RDD**

DataFrame

Dataset

All of the above

**RDD in Apache Spark is a low-level data structure that provides a distributed collection of data objects. RDDs allow for low-level transformations and actions, providing more control and flexibility in data processing operations. RDDs are suitable for scenarios where fine-grained control is required, such as custom algorithms or specialized data processing tasks. However, DataFrame and Dataset APIs provide higher-level abstractions that are generally more efficient and convenient for common data manipulation tasks, offering optimized execution plans and built-in optimizations for query processing.**


## 104.- Which of the following technology is good for Stream technology?

Apache Spark

Apache Hadoop

**Apache Flink**

None of the above

**Apache Flink is a powerful stream processing framework designed for high-throughput, low-latency, and fault-tolerant stream processing of big data. It provides advanced capabilities for processing continuous streams of data with support for event time processing, windowing, stateful computations, and fault tolerance. Apache Spark, on the other hand, is primarily a batch processing and real-time analytics framework, although it does have some support for streaming data processing through its structured streaming module. Apache Hadoop is a distributed processing framework that primarily focuses on batch processing and storage of large datasets, rather than real-time stream processing.**

## 105.- Which of the following is not true for Apache Spark Execution?

To simplify working with structured data it provides DataFrame abstraction in Python, Java, and Scala.

The data can be read and written in a variety of structured formats. For example, JSON, Hive Tables, and Parquet.

**Using SQL we can query data,only from inside a Spark program and not from external tools.**

The best way to use Spark SQL is inside a Spark application. This empowers us to load data and query it with SQL.

**"Using SQL we can query data, only from inside a Spark program and not from external tools" is not true for Apache Spark Execution. Apache Spark allows us to use SQL to query data both from inside a Spark program and from external tools. Spark provides a SQL interface that can be used to run SQL queries directly on DataFrames or temporary views created from the data. This allows users to leverage their SQL skills and tools to interact with Spark data efficiently.**

## 106.- When SQL run from the other programming language the result will be  ***

DataFrame

DataSet

**Either DataFrame or Dataset**

Neither DataFrame nor Dataset


## 107.- The Dataset API is accessible in

**Java and Scala**

Java, Scala and python

Scala and Python

Scala and R


## 108.- Dataset API is not supported by Python. But because of the dynamic nature of Python, many benefits of Dataset API are available.

True

**False**

## 109.- Which of the following is true for Catalyst optimizer?

**The optimizer helps us to run queries much faster than their counter RDD part.**

The optimizer helps us to run queries little faster than their counter RDD part.

The optimizer helps us to run queries in the same speed as their counter RDD part.


## 110.- Which of the following organized a data into a named column?  

a. RDD  b. DataFrame  c. Dataset

**Both a and b**
Both b and c
Both a and c

**The correct answer is option a. Both a. RDD and b. DataFrame. RDDs can be transformed into DataFrames, which provide a structured and named columnar format for organizing data.**

## 111.- Which of the following are uses of Apache Spark SQL?

It executes SQL queries.

We can read data from existing Hive installation using SparkSQL.

When we run SQL within another programming language we will get the result as Dataset/DataFrame.

**All of the above**

**"All of the above." Apache Spark SQL allows you to execute SQL queries, read data from existing Hive installations using SparkSQL, and retrieve the result as a Dataset or DataFrame when running SQL within another programming language.**

## 112.- With the help of Spark SQL, we can query structured data as a distributed dataset (RDD).

True

**False**

**False. With the help of Spark SQL, we can query structured data as a distributed Dataset or DataFrame, not as an RDD. Spark SQL provides a higher-level interface that allows for more efficient and optimized querying of structured data.**

## 113.- Spark SQL can connect through JDBC or ODBC.

**True**

False

**True. Spark SQL provides connectivity options through JDBC (Java Database Connectivity) and ODBC (Open Database Connectivity), allowing users to connect to various external data sources and query them using Spark SQL.**

## 114.- Using Spark SQL, we can create or read a table containing union fields.

True

**False**

**In Spark SQL, the concept of union fields is not supported. Union fields refer to combining multiple tables or datasets with potentially different column structures. While Spark SQL allows performing operations like union and unionAll to combine tables or datasets, the column structures must match. If the column structures are different, you may need to perform data transformation or schema alignment before performing the union operation.**

## 115.- Which of the following is true for Spark SQL? (false??)  ***

Hive transactions are not supported by Spark SQL.

No support for time-stamp in Avro table.

Even if the inserted value exceeds the size limit, no error will occur.

**All of the above(*)**


NOTA: Si la pregunta dijera cual de las siguientes es falsa entonces si la respuesta serÃ¬a

**None of the statements is true for Spark SQL.**

**Spark SQL does support Hive transactions, allowing you to perform ACID (Atomicity, Consistency, Isolation, Durability) operations on data stored in Hive tables.Spark SQL supports timestamps in Avro tables.**
**Timestamps can be stored and queried in Avro data using Spark SQL. If the inserted value exceeds the size limit, an error will occur based on the data type and column constraints defined in the table schema. Spark SQL enforces data type validation and integrity constraints during data insertion.**
**Therefore, none of the options presented is correct.**

